{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene expression prediction (CNN version)\n",
    "Reference: https://www.kaggle.com/c/gene-expression-prediction\n",
    "\n",
    "Coded by Wangjin Lee\n",
    "- jinsamdol@snu.ac.kr \n",
    "\n",
    "#### Description\n",
    "Histone modifications are playing an important role in affecting gene regulation. Nowadays, predicting gene expression from histone modification signals is a widely studied research topic.\n",
    "\n",
    "The dataset of this competition is on \"E047\" (Primary T CD8+ naive cells from peripheral blood) celltype from Roadmap Epigenomics Mapping Consortium (REMC) database. For each gene, it has 100 bins with five core histone modification marks [1]. (We divide the 10,000 basepair(bp) DNA region (+/-5000bp) around the transcription start site (TSS) of each gene into bins of length 100 bp [2], and then count the reads of 100 bp in each bin. Finally, the signal of each gene has a shape of 100x5.)\n",
    "\n",
    "The goal of this competition is to develop algorithms for accurate predicting gene expression level. High gene expression level corresponds to target label = 1, and low gene expression corresponds to target label = 0.\n",
    "\n",
    "Thus, the inputs are 100x5 matrices and target is the probability of gene activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "\n",
    "*In this lesson, we will only use **training set**. The set consists of 'x_train.csv' and 'y_train.csv'.*\n",
    "\n",
    "**x_train.csv**\n",
    "\n",
    "This file contains the feature of the genes\n",
    "\n",
    "A single gene (Field name: **GeneId**) has 500 reads information (shape: 100 x 5 ; 100 bp and 5 histones). The value in each cell for the histone denotes the number of reads in the read cluster.\n",
    "\n",
    "The total number of the genes : 15,485\n",
    "\n",
    "x_train.csv file looks like this ..\n",
    "\n",
    "| GeneId | H3K4me3 | H3K4me1 | H3K36me3 | H3K9me3 | H3K27me3 |\n",
    "|---|---|---|---|---|---|\n",
    "| 1 | 2 | 1 | 4 | 1 | 0 |\n",
    "| 1 | 0 | 2 | 1 | 1 | 1 |\n",
    "| ... |\n",
    "| 2  | 1 | 0 | 1 | 0 | 0 |\n",
    "| ...  |\n",
    "| 15485  | 0 | 0 | 0 | 2 | 1 |\n",
    "\n",
    "The total number of the rows: 1,548,501 (15485 x 100 + 1)\n",
    "\n",
    "The total number of the columns: 6 (1 + 5)\n",
    "\n",
    "**x_train.csv**\n",
    "\n",
    "This file contains the prediction whether a gene is expressed or not.\n",
    "\n",
    "One prediction label for one GeneId.\n",
    "Prediction label is binary (1: expressed, 0: suppressed)\n",
    "\n",
    "\n",
    "| GeneId | Prediction |\n",
    "|---|---|\n",
    "| 1 | 0 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| ... |\n",
    "| 15485 | 0 |\n",
    "\n",
    "The total number of the rows: 15,486 (15485 + 1)\n",
    "\n",
    "The total number of the columns: 2 (1 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Curate a Dataset\n",
    "The cells from here until **Loading the preprocessed data** include code for loading the data from the training files and storing them into variables, and dumping them into a preprocess file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign variable for each the data file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path_x = os.path.abspath('./data/gene_expression/train/x_train.csv')\n",
    "data_path_y = os.path.abspath('./data/gene_expression/train/y_train.csv')\n",
    "print(data_path_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the x_train.csv file via pandas.read_csv( ) operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = pd.read_csv(data_path_x, delimiter = ',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the some first rows in the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_x.drop(columns=['GeneId'])\n",
    "data_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the y_train.csv file via pandas.read_csv( ) operation\n",
    "\n",
    "The prediction is on the second column (usecols=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = pd.read_csv(data_path_y, delimiter = ',', header=0, usecols=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the some first rows in the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y2 = data_y.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how the data_y2 looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_y2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the preprocessed data via pickle.dump( ) operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocess_ge_cnn.p', 'wb') as out_file:\n",
    "    pickle.dump((data_x, data_y2), out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Load the dataset\n",
    "\n",
    "#### After storing the preprocessed data into a file, we can begin at here and save our time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_preprocess():\n",
    "    with open('preprocess_ge_cnn.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "(all_data_x, all_data_y) = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to scale (normalize) the x_data. \n",
    "Through the output from the data_x.describe() above, we checked that the data in each cell has large variance. It may lead poor prediction performance in neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(all_data_x).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548500 15485 1548500\n",
      "[[0.17557490236948337, -0.2932485067426945, 0.13486841189835097, -0.3322358747505786, -0.7980722505108142], [-0.7690025785016734, 0.2563978841954451, -0.5056471723258046, -0.3322358747505786, -0.08486416286080267]]\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data_x), len(all_data_y), len(scaled_x))\n",
    "print(scaled_x[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data to CNN input style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cnn_x = []\n",
    "bp = 100\n",
    "for i in range(len(all_data_y)):\n",
    "    #a = np.reshape(scaled_x[i*bp : (i+1)*bp], (100,5))\n",
    "    data_cnn_x.append(np.reshape(scaled_x[i*bp : (i+1)*bp], (100,5,1)).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15485, 100, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data_cnn_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "The label in a binary or n-ary classification problem can be encoded via One-Hot Encoding method.\n",
    "One-Hot encoding for binary classification problem can be seen as a method encoding label in 2-dimensional vector. \n",
    "\n",
    "Example: \n",
    "\n",
    "|Prediction |\n",
    "|---|\n",
    "|0|\n",
    "|1|\n",
    "|0|\n",
    "|0|\n",
    "\n",
    "- 1-dimensional vector: [[0], [1], [0], [0]]\n",
    "\n",
    "- One-hot encoding (2-dimensional vector): [[1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "\n",
    "Each dimension in the 2-d vector denotes each element, thus, the *n* is larger, more zeros in the encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [1], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "#all_data_y_list = all_data_y.tolist()\n",
    "print(all_data_y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_y = OneHotEncoder(sparse=False).fit_transform(all_data_y).tolist()\n",
    "print(data_y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the data → Training / Development / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the X-Y pair and the number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15485\n"
     ]
    }
   ],
   "source": [
    "n_total = len(data_cnn_x)\n",
    "print(n_total)\n",
    "n_total_y = len(data_y)\n",
    "assert n_total == n_total_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning!!: In the case you want to use small pieces of the data set, run this cell below, or, skip this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using only 1/n of the whole data\n",
    "'''\n",
    "import math\n",
    "_n=3\n",
    "data_cnn_x = data_cnn_x[:math.floor(n_total/_n)]\n",
    "data_y = data_y[:math.floor(n_total/_n)]\n",
    "\n",
    "n_total = len(data_cnn_x)\n",
    "print(n_total)\n",
    "n_total_y = len(data_y)\n",
    "assert n_total == n_total_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide the whole set into 7:1:2 for training, dev, and test, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train:    10836\n",
      "#dev:    1548\n",
      "#test:    3101\n"
     ]
    }
   ],
   "source": [
    "x_data_train = data_cnn_x[:n_total//10*7]\n",
    "y_data_train = data_y[:n_total//10*7]\n",
    "\n",
    "x_data_dev = data_cnn_x[n_total//10*7:n_total//10*8]\n",
    "y_data_dev = data_y[n_total//10*7:n_total//10*8]\n",
    "\n",
    "x_data_test = data_cnn_x[n_total//10*8:]\n",
    "y_data_test = data_y[n_total//10*8:]\n",
    "print ('#train: ',' ', len(x_data_train))\n",
    "print ('#dev: ',' ', len(x_data_dev))\n",
    "print ('#test: ',' ', len(x_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10836, 100, 5, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "x_col = 5\n",
    "x_row = 100\n",
    "x_cell = x_row*x_col\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "* We will design a neural network graph consisting of one input layer, three hidden layers, and one output layer.\n",
    "* In the cell below, set your hyper parameters for the node number for each layer\n",
    "* Input layer (size 500) → Layer 1 (size ?) → Activation → Dropout → Layer 2 (size ?) → Activation → Dropout → Layer 3 (size ?) → Activation → Output layer (size ?) → Softmax\n",
    "* Also, we can control the learning rate and *keep probability* in the dropout layer. Set your hyperparameters for the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.001 #learning rate\n",
    "kp=0.4 #keep probability (dropout layer)\n",
    "n_iter=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the graph structure in the TF code. \n",
    "You may use this structure and can modify the structure.\n",
    "* tf.name_scope('name') will make a group in the graph. It is very useful tip making your graph compact when you see your graph in the tensorboard.\n",
    "* If you want to see scalar (cost), use **tf.summary.scalar('name', variable)**\n",
    "* If you want to see histogram (weights, bias), use **tf.summary.histogram('name', variable)**\n",
    "* Before running the training session, type **your_summary_name=tf.summary.merge_all()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print('x_tensor.shape: ', x_tensor.shape)\n",
    "    n_input_feature_map = x_tensor.get_shape()[3].value\n",
    "    #print('feature_map: ', n_input_feature_map)\n",
    "    \n",
    "    #print('conv_num_outputs: ', conv_num_outputs)\n",
    "    #print('conv_ksize: ', conv_ksize)\n",
    "    #print('conv_strides: ', conv_strides)\n",
    "    #print('pool_ksize: ', pool_ksize)\n",
    "    #print('pool_strides: ', pool_strides)\n",
    "    \n",
    "    #Create the weight and bias using conv_ksize, conv_num_outputs and the shape of x_tensor    \n",
    "    W = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], n_input_feature_map, conv_num_outputs], mean = 0.0, stddev=0.05),\\\n",
    "                    name='weight')\n",
    "    #print('shape_of_the_weight: ', W.shape)\n",
    "    \n",
    "    b = tf.Variable(tf.zeros(conv_num_outputs), dtype=tf.float32, name='bias')\n",
    "    #print('shape_of_the_bias: ', b.shape)\n",
    "    \n",
    "    #apply a convolution to x_tensor using weight and conv_strides (with same padding)\n",
    "    convolution = tf.nn.conv2d(x_tensor, W, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')        \n",
    "    convolution = tf.nn.bias_add(convolution, b)#add bias        \n",
    "    convolution = tf.nn.relu(convolution)#add a nonlinear activation to the convolution\n",
    "    \n",
    "    #apply max pooling using pool_ksize and pool_strides\n",
    "    convolution = tf.nn.max_pool(convolution, ksize=[1, pool_ksize[0], pool_ksize[1], 1],\\\n",
    "                                 strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    return convolution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.shape)\n",
    "    \n",
    "    W = x_tensor.get_shape()[1].value    \n",
    "    H = x_tensor.get_shape()[2].value    \n",
    "    D = x_tensor.get_shape()[3].value\n",
    "    \n",
    "    #print ('W:', W, ' H:', H, ' D:', D, ' #:', W*H*D)\n",
    "    \n",
    "    flatten = tf.reshape(x_tensor, [-1, W*H*D])\n",
    "    \n",
    "    \n",
    "    #print(flatten.shape)\n",
    "    return flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.get_shape()[1].value)\n",
    "    W = tf.Variable(tf.truncated_normal( [x_tensor.get_shape()[1].value, num_outputs] , mean = 0.0, stddev=0.05),  name='weight')\n",
    "    b = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32, name='bias')\n",
    "    \n",
    "    fc = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    fc = tf.nn.relu(fc)\n",
    "    return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply an output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    #print(x_tensor.get_shape()[1].value)\n",
    "    W = tf.Variable(tf.truncated_normal([x_tensor.get_shape()[1].value, num_outputs], mean = 0.0, stddev=0.05), name='weight')\n",
    "    b = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32, name='bias')\n",
    "    \n",
    "    out = tf.add(tf.matmul(x_tensor, W), b)\n",
    "    \n",
    "    #without softmax\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "    num_classes = 2\n",
    "    \n",
    "    #layer 1\n",
    "    conv_num_outputs1 = 50\n",
    "    conv_ksize1 = (3, 3)\n",
    "    conv_strides1 = (1, 1)\n",
    "    pool_ksize1 = (2, 2)\n",
    "    pool_strides1 = (2, 2)\n",
    "           \n",
    "    conv_layer1 = conv2d_maxpool(x, conv_num_outputs1, conv_ksize1, conv_strides1, pool_ksize1, pool_strides1)\n",
    "    conv_layer1 = tf.nn.dropout(conv_layer1, keep_prob)\n",
    "    \n",
    "    #layer 2\n",
    "    conv_num_outputs2 = 24\n",
    "    conv_ksize2 = (3, 3)\n",
    "    conv_strides2 = (1, 1)\n",
    "    pool_ksize2 = (2, 2)\n",
    "    pool_strides2 = (2, 2)\n",
    "    \n",
    "    conv_layer2 = conv2d_maxpool(conv_layer1, conv_num_outputs2, conv_ksize2, conv_strides2, pool_ksize2, pool_strides2)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat_layer = flatten(conv_layer2)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fc = fully_conn(flat_layer, 12)\n",
    "    fc = tf.nn.dropout(fc, keep_prob=keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    out = output(fc, num_classes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #layer 1\n",
    "    conv_num_outputs1 = 30\n",
    "    conv_ksize1 = (3, 3)\n",
    "    conv_strides1 = (1, 1)\n",
    "    pool_ksize1 = (2, 2)\n",
    "    pool_strides1 = (2, 2)\n",
    "           \n",
    "    #layer 2\n",
    "    conv_num_outputs2 = 10\n",
    "    conv_ksize2 = (3, 3)\n",
    "    conv_strides2 = (1, 1)\n",
    "    pool_ksize2 = (2, 2)\n",
    "    pool_strides2 = (1, 1)\n",
    "    \n",
    "    flat_layer = flatten(conv_layer2)\n",
    "    \n",
    "\n",
    "    \n",
    "    fc = fully_conn(flat_layer, 5)\n",
    "    fc = tf.nn.dropout(fc, keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #layer 1\n",
    "    conv_num_outputs1 = 10\n",
    "    conv_ksize1 = (1, 3)\n",
    "    conv_strides1 = (1, 1)\n",
    "    pool_ksize1 = (1, 1)\n",
    "    pool_strides1 = (1, 1)\n",
    "           \n",
    "   \n",
    "    #layer 2\n",
    "    conv_num_outputs2 = 5\n",
    "    conv_ksize2 = (1, 3)\n",
    "    conv_strides2 = (1, 1)\n",
    "    pool_ksize2 = (1, 1)\n",
    "    pool_strides2 = (1, 1)\n",
    "    \n",
    "    conv_layer2 = conv2d_maxpool(conv_layer1, conv_num_outputs2, conv_ksize2, conv_strides2, pool_ksize2, pool_strides2)\n",
    "    \n",
    "   \n",
    "    flat_layer = flatten(conv_layer2)\n",
    "    \n",
    "    fc = fully_conn(flat_layer, 3)\n",
    "    fc = tf.nn.dropout(fc, keep_prob=keep_prob)\n",
    "    out = output(fc, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "double layers\n",
    "'''\n",
    "\n",
    "ncol = len(scaled_x[0])\n",
    "nclass = n_classes\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    \n",
    "    #X = tf.placeholder(tf.float32, shape=[None, x_cell], name='x')\n",
    "    #X_img = tf.reshape(X, shape=[None, x_col, x_row, 1], name='x_image')\n",
    "    X = tf.placeholder(tf.float32, shape=(None, x_row, x_col, 1), name='x')\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, nclass], name='y')\n",
    "\n",
    "\n",
    "    \n",
    "with tf.name_scope('conv'):\n",
    "    # Model\n",
    "    logits = conv_net(X, kp)\n",
    "\n",
    "    # Name logits Tensor, so that is can be loaded from disk after training\n",
    "    logits = tf.identity(logits, name='logits')\n",
    "\n",
    "with tf.name_scope('trainer'):\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels = Y))    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)        \n",
    "    \n",
    "with tf.name_scope('predictor'):\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean (tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to save tensorboard log file, type like this just after sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  **file_name = tf.summary.FileWriter('log_folder_name', sess.graph)**\n",
    "\n",
    "* If your want to save the variables during the training, run the code\n",
    "\n",
    "  **summary = sess.run([your_summary_name], feed_dict={....})**\n",
    "  **file_name.add_summary(summary, step)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tcost: 0.6988085 \tacc: 0.48901808\n",
      "200 \tcost: 0.44161975 \tacc: 0.7209302\n",
      "400 \tcost: 0.41314155 \tacc: 0.7661499\n",
      "600 \tcost: 0.38850668 \tacc: 0.78488374\n",
      "800 \tcost: 0.36614487 \tacc: 0.7661499\n",
      "1000 \tcost: 0.35158038 \tacc: 0.7667959\n",
      "1200 \tcost: 0.33678615 \tacc: 0.77131784\n",
      "1400 \tcost: 0.33094656 \tacc: 0.751938\n",
      "1600 \tcost: 0.31489202 \tacc: 0.74547803\n",
      "1800 \tcost: 0.31130528 \tacc: 0.748708\n",
      "2000 \tcost: 0.31111312 \tacc: 0.7629199\n",
      "2200 \tcost: 0.30164725 \tacc: 0.75775194\n",
      "2400 \tcost: 0.29535374 \tacc: 0.77067184\n",
      "2600 \tcost: 0.2822594 \tacc: 0.76937985\n",
      "2800 \tcost: 0.2714449 \tacc: 0.75775194\n",
      "3000 \tcost: 0.27218685 \tacc: 0.7629199\n",
      "3200 \tcost: 0.267765 \tacc: 0.7648579\n",
      "3400 \tcost: 0.2640853 \tacc: 0.7609819\n",
      "3600 \tcost: 0.26451153 \tacc: 0.751292\n",
      "3800 \tcost: 0.26865396 \tacc: 0.7655039\n",
      "4000 \tcost: 0.25838754 \tacc: 0.7732558\n",
      "4200 \tcost: 0.25614098 \tacc: 0.7751938\n",
      "4400 \tcost: 0.2649098 \tacc: 0.76937985\n",
      "4600 \tcost: 0.2513209 \tacc: 0.75775194\n",
      "4800 \tcost: 0.26363042 \tacc: 0.7596899\n",
      "5000 \tcost: 0.2525931 \tacc: 0.77067184\n",
      "final accuracy:  0.77067184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #for 1 layer\n",
    "    fw2 = tf.summary.FileWriter('./tb/gene_expression_cnn/train/model{}'.format(3), sess.graph)\n",
    "    fw_val = tf.summary.FileWriter('./tb/gene_expression_cnn/validation/model{}'.format(3))\n",
    "    fw_test = tf.summary.FileWriter('./tb/gene_expression_cnn/test/model{}'.format(3))\n",
    "    \n",
    "\n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        \n",
    "        summary, _, c = sess.run([merged, optimizer, cost], feed_dict={X:x_data_train, Y:y_data_train})\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            summary_v, acc = sess.run([merged, accuracy], feed_dict={X:x_data_dev, Y:y_data_dev})\n",
    "            print(step, '\\tcost:', c, '\\tacc:', acc)\n",
    "            #print(summary_v)\n",
    "            fw_val.add_summary(summary_v, step)\n",
    "            \n",
    "        fw2.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        \n",
    "    summary_t, acc_t = sess.run([merged, accuracy], feed_dict={X:x_data_test, Y:y_data_test})    \n",
    "    fw_test.add_summary(summary_t, step)\n",
    "    print('final accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
