{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene expression prediction\n",
    "Reference: https://www.kaggle.com/c/gene-expression-prediction\n",
    "\n",
    "Coded by Wangjin Lee\n",
    "- jinsamdol@snu.ac.kr \n",
    "\n",
    "#### Description\n",
    "Histone modifications are playing an important role in affecting gene regulation. Nowadays, predicting gene expression from histone modification signals is a widely studied research topic.\n",
    "\n",
    "The dataset of this competition is on \"E047\" (Primary T CD8+ naive cells from peripheral blood) celltype from Roadmap Epigenomics Mapping Consortium (REMC) database. For each gene, it has 100 bins with five core histone modification marks [1]. (We divide the 10,000 basepair(bp) DNA region (+/-5000bp) around the transcription start site (TSS) of each gene into bins of length 100 bp [2], and then count the reads of 100 bp in each bin. Finally, the signal of each gene has a shape of 100x5.)\n",
    "\n",
    "The goal of this competition is to develop algorithms for accurate predicting gene expression level. High gene expression level corresponds to target label = 1, and low gene expression corresponds to target label = 0.\n",
    "\n",
    "Thus, the inputs are 100x5 matrices and target is the probability of gene activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "\n",
    "*In this lesson, we will only use **training set**. The set consists of 'x_train.csv' and 'y_train.csv'.*\n",
    "\n",
    "**x_train.csv**\n",
    "\n",
    "This file contains the feature of the genes\n",
    "\n",
    "A single gene (Field name: **GeneId**) has 500 reads information (shape: 100 x 5 ; 100 bp and 5 histones). The value in each cell for the histone denotes the number of reads in the read cluster.\n",
    "\n",
    "The total number of the genes : 15,485\n",
    "\n",
    "x_train.csv file looks like this ..\n",
    "\n",
    "| GeneId | H3K4me3 | H3K4me1 | H3K36me3 | H3K9me3 | H3K27me3 |\n",
    "|---|---|---|---|---|---|\n",
    "| 1 | 2 | 1 | 4 | 1 | 0 |\n",
    "| 1 | 0 | 2 | 1 | 1 | 1 |\n",
    "| ... |\n",
    "| 2  | 1 | 0 | 1 | 0 | 0 |\n",
    "| ...  |\n",
    "| 15485  | 0 | 0 | 0 | 2 | 1 |\n",
    "\n",
    "The total number of the rows: 1,548,501 (15485 x 100 + 1)\n",
    "\n",
    "The total number of the columns: 6 (1 + 5)\n",
    "\n",
    "**x_train.csv**\n",
    "\n",
    "This file contains the prediction whether a gene is expressed or not.\n",
    "\n",
    "One prediction label for one GeneId.\n",
    "Prediction label is binary (1: expressed, 0: suppressed)\n",
    "\n",
    "\n",
    "| GeneId | Prediction |\n",
    "|---|---|\n",
    "| 1 | 0 |\n",
    "| 2 | 0 |\n",
    "| 3 | 1 |\n",
    "| ... |\n",
    "| 15485 | 0 |\n",
    "\n",
    "The total number of the rows: 15,486 (15485 + 1)\n",
    "\n",
    "The total number of the columns: 2 (1 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Curate a Dataset\n",
    "The cells from here until **Loading the preprocessed data** include code for loading the data from the training files and storing them into variables, and dumping them into a preprocess file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign variable for each the data file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\206_python\\tensor_board_ex\\data\\gene_expression\\train\\x_train.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path_x = os.path.abspath('./data/gene_expression/train/x_train.csv')\n",
    "data_path_y = os.path.abspath('./data/gene_expression/train/y_train.csv')\n",
    "print(data_path_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the x_train.csv file via pandas.read_csv( ) operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = pd.read_csv(data_path_x, delimiter = ',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeneId</th>\n",
       "      <th>H3K4me3</th>\n",
       "      <th>H3K4me1</th>\n",
       "      <th>H3K36me3</th>\n",
       "      <th>H3K9me3</th>\n",
       "      <th>H3K27me3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.548500e+06</td>\n",
       "      <td>1.548500e+06</td>\n",
       "      <td>1.548500e+06</td>\n",
       "      <td>1.548500e+06</td>\n",
       "      <td>1.548500e+06</td>\n",
       "      <td>1.548500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.743000e+03</td>\n",
       "      <td>1.628247e+00</td>\n",
       "      <td>1.533522e+00</td>\n",
       "      <td>3.368313e+00</td>\n",
       "      <td>5.099660e+00</td>\n",
       "      <td>1.118989e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.470136e+03</td>\n",
       "      <td>2.117349e+00</td>\n",
       "      <td>1.819352e+00</td>\n",
       "      <td>4.683728e+00</td>\n",
       "      <td>1.233961e+01</td>\n",
       "      <td>1.402116e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.872000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.743000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.161400e+04</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.548500e+04</td>\n",
       "      <td>1.630000e+02</td>\n",
       "      <td>9.300000e+01</td>\n",
       "      <td>1.060000e+02</td>\n",
       "      <td>1.610000e+02</td>\n",
       "      <td>1.700000e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             GeneId       H3K4me3       H3K4me1      H3K36me3       H3K9me3  \\\n",
       "count  1.548500e+06  1.548500e+06  1.548500e+06  1.548500e+06  1.548500e+06   \n",
       "mean   7.743000e+03  1.628247e+00  1.533522e+00  3.368313e+00  5.099660e+00   \n",
       "std    4.470136e+03  2.117349e+00  1.819352e+00  4.683728e+00  1.233961e+01   \n",
       "min    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    3.872000e+03  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    7.743000e+03  1.000000e+00  1.000000e+00  2.000000e+00  1.000000e+00   \n",
       "75%    1.161400e+04  2.000000e+00  2.000000e+00  4.000000e+00  3.000000e+00   \n",
       "max    1.548500e+04  1.630000e+02  9.300000e+01  1.060000e+02  1.610000e+02   \n",
       "\n",
       "           H3K27me3  \n",
       "count  1.548500e+06  \n",
       "mean   1.118989e+00  \n",
       "std    1.402116e+00  \n",
       "min    0.000000e+00  \n",
       "25%    0.000000e+00  \n",
       "50%    1.000000e+00  \n",
       "75%    2.000000e+00  \n",
       "max    1.700000e+02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the some first rows in the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeneId</th>\n",
       "      <th>H3K4me3</th>\n",
       "      <th>H3K4me1</th>\n",
       "      <th>H3K36me3</th>\n",
       "      <th>H3K9me3</th>\n",
       "      <th>H3K27me3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GeneId  H3K4me3  H3K4me1  H3K36me3  H3K9me3  H3K27me3\n",
       "0       1        2        1         4        1         0\n",
       "1       1        0        2         1        1         1\n",
       "2       1        0        0         4        1         1\n",
       "3       1        0        2         2        0         1\n",
       "4       1        2        0         0        0         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will store the loaded data (x) into a list.**\n",
    "\n",
    "The read information for a histone in a single gene is separated in 100 consecutive rows, in addition, a gene has the data for histones.\n",
    "\n",
    "We will transform the 2-dimension data to 1-dimension.\n",
    "\n",
    "*2d*\n",
    "\n",
    "| GeneId | Histone1 | Histone2 | Histone3 | Histone4 | Histone5 |\n",
    "|---|---|---|---|---|---|\n",
    "| 1 | 1a | 1d | 1g | 1j | 1m |\n",
    "| 1 | 1b | 1e | 1h | 1k | 1n |\n",
    "| 1 | 1c | 1f | 1i | 1l | 1o |\n",
    "| 2 | 2a | 2d | 2g | 2j | 2m |\n",
    "| 2 | 2b | 2e | 2h | 2k | 2n |\n",
    "| 2 | 2c | 2f | 2i | 2l | 2o |\n",
    "| ... |\n",
    "\n",
    "to *1d* \n",
    "\n",
    "| GeneId | Histone1-1 | Histone1-2 | Histone1-3 | Histone2-1 | ... | Histone5-3 |\n",
    "|---|---|---|---|---|---|---|\n",
    "| 1 | 1a | 1b | 1c | 1d | ... | 1o |\n",
    "| 2 | 2a | 2b | 2c | 2d | ... | 2o |\n",
    "| ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "processed:  100\n",
      "processed:  200\n",
      "processed:  300\n",
      "processed:  400\n",
      "processed:  500\n",
      "processed:  600\n",
      "processed:  700\n",
      "processed:  800\n",
      "processed:  900\n",
      "processed:  1000\n",
      "processed:  1100\n",
      "processed:  1200\n",
      "processed:  1300\n",
      "processed:  1400\n",
      "processed:  1500\n",
      "processed:  1600\n",
      "processed:  1700\n",
      "processed:  1800\n",
      "processed:  1900\n",
      "processed:  2000\n",
      "processed:  2100\n",
      "processed:  2200\n",
      "processed:  2300\n",
      "processed:  2400\n",
      "processed:  2500\n",
      "processed:  2600\n",
      "processed:  2700\n",
      "processed:  2800\n",
      "processed:  2900\n",
      "processed:  3000\n",
      "processed:  3100\n",
      "processed:  3200\n",
      "processed:  3300\n",
      "processed:  3400\n",
      "processed:  3500\n",
      "processed:  3600\n",
      "processed:  3700\n",
      "processed:  3800\n",
      "processed:  3900\n",
      "processed:  4000\n",
      "processed:  4100\n",
      "processed:  4200\n",
      "processed:  4300\n",
      "processed:  4400\n",
      "processed:  4500\n",
      "processed:  4600\n",
      "processed:  4700\n",
      "processed:  4800\n",
      "processed:  4900\n",
      "processed:  5000\n",
      "processed:  5100\n",
      "processed:  5200\n",
      "processed:  5300\n",
      "processed:  5400\n",
      "processed:  5500\n",
      "processed:  5600\n",
      "processed:  5700\n",
      "processed:  5800\n",
      "processed:  5900\n",
      "processed:  6000\n",
      "processed:  6100\n",
      "processed:  6200\n",
      "processed:  6300\n",
      "processed:  6400\n",
      "processed:  6500\n",
      "processed:  6600\n",
      "processed:  6700\n",
      "processed:  6800\n",
      "processed:  6900\n",
      "processed:  7000\n",
      "processed:  7100\n",
      "processed:  7200\n",
      "processed:  7300\n",
      "processed:  7400\n",
      "processed:  7500\n",
      "processed:  7600\n",
      "processed:  7700\n",
      "processed:  7800\n",
      "processed:  7900\n",
      "processed:  8000\n",
      "processed:  8100\n",
      "processed:  8200\n",
      "processed:  8300\n",
      "processed:  8400\n",
      "processed:  8500\n",
      "processed:  8600\n",
      "processed:  8700\n",
      "processed:  8800\n",
      "processed:  8900\n",
      "processed:  9000\n",
      "processed:  9100\n",
      "processed:  9200\n",
      "processed:  9300\n",
      "processed:  9400\n",
      "processed:  9500\n",
      "processed:  9600\n",
      "processed:  9700\n",
      "processed:  9800\n",
      "processed:  9900\n",
      "processed:  10000\n",
      "processed:  10100\n",
      "processed:  10200\n",
      "processed:  10300\n",
      "processed:  10400\n",
      "processed:  10500\n",
      "processed:  10600\n",
      "processed:  10700\n",
      "processed:  10800\n",
      "processed:  10900\n",
      "processed:  11000\n",
      "processed:  11100\n",
      "processed:  11200\n",
      "processed:  11300\n",
      "processed:  11400\n",
      "processed:  11500\n",
      "processed:  11600\n",
      "processed:  11700\n",
      "processed:  11800\n",
      "processed:  11900\n",
      "processed:  12000\n",
      "processed:  12100\n",
      "processed:  12200\n",
      "processed:  12300\n",
      "processed:  12400\n",
      "processed:  12500\n",
      "processed:  12600\n",
      "processed:  12700\n",
      "processed:  12800\n",
      "processed:  12900\n",
      "processed:  13000\n",
      "processed:  13100\n",
      "processed:  13200\n",
      "processed:  13300\n",
      "processed:  13400\n",
      "processed:  13500\n",
      "processed:  13600\n",
      "processed:  13700\n",
      "processed:  13800\n",
      "processed:  13900\n",
      "processed:  14000\n",
      "processed:  14100\n",
      "processed:  14200\n",
      "processed:  14300\n",
      "processed:  14400\n",
      "processed:  14500\n",
      "processed:  14600\n",
      "processed:  14700\n",
      "processed:  14800\n",
      "processed:  14900\n",
      "processed:  15000\n",
      "processed:  15100\n",
      "processed:  15200\n",
      "processed:  15300\n",
      "processed:  15400\n"
     ]
    }
   ],
   "source": [
    "# basepair information\n",
    "bp = 100\n",
    "n_gene = 15485\n",
    "n_histone = 5\n",
    "\n",
    "#15485x(100*5) list. This list will store the 1d data\n",
    "data_all_x = [[0]*(n_histone*bp) for i in range(n_gene)]\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "col4 = []\n",
    "col5 = []\n",
    "\n",
    "push_idx = 0\n",
    "print ('start')\n",
    "for index, row in data_x.iterrows():\n",
    "\n",
    "    if (index+1)%bp == 0:\n",
    "        \n",
    "        col1.append(row['H3K4me3'])\n",
    "        col2.append(row['H3K4me1'])\n",
    "        col3.append(row['H3K36me3'])\n",
    "        col4.append(row['H3K9me3'])\n",
    "        col5.append(row['H3K27me3'])\n",
    "        \n",
    "        #print(col1)\n",
    "        #print(data_all[push_idx])\n",
    "        data_all_x[push_idx] = list(itertools.chain(*[col1, col2, col3, col4, col5]))\n",
    "        \n",
    "        push_idx += 1\n",
    "    \n",
    "        col1 = []\n",
    "        col2 = []\n",
    "        col3 = []\n",
    "        col4 = []\n",
    "        col5 = []\n",
    "        #print ('clear')\n",
    "        if push_idx%100==0:\n",
    "            print ('processed: ', push_idx)\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        col1.append(row['H3K4me3'])\n",
    "        col2.append(row['H3K4me1'])\n",
    "        col3.append(row['H3K36me3'])\n",
    "        col4.append(row['H3K9me3'])\n",
    "        col5.append(row['H3K27me3'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first data:\n",
      "[2, 0, 0, 0, 2, 1, 2, 1, 2, 0, 2, 0, 2, 1, 1, 0, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 3, 2, 3, 3, 2, 3, 1, 0, 0, 2, 0, 3, 4, 2, 0, 6, 3, 1, 3, 5, 5, 5, 2, 3, 1, 3, 0, 1, 0, 2, 1, 1, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 1, 0, 0, 2, 1, 0, 1, 4, 4, 3, 1, 1, 2, 0, 0, 3, 3, 0, 3, 1, 1, 1, 0, 0, 2, 4, 1, 1, 3, 1, 2, 4, 1, 2, 0, 2, 0, 2, 2, 1, 3, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 3, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 4, 2, 1, 0, 0, 0, 3, 2, 0, 1, 2, 1, 2, 0, 1, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 2, 1, 4, 1, 4, 2, 0, 0, 2, 4, 4, 3, 3, 4, 5, 3, 0, 1, 2, 2, 3, 2, 3, 4, 3, 1, 1, 4, 7, 3, 4, 3, 3, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 2, 5, 2, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 2, 3, 2, 4, 1, 3, 9, 8, 14, 15, 12, 12, 7, 4, 5, 5, 2, 3, 2, 4, 4, 4, 1, 1, 1, 3, 2, 0, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 2, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 2, 1, 1, 1, 2, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 3, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 1, 1, 0, 1, 0, 4, 1, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 1, 1, 1, 3, 3, 0, 4, 2, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1]\n",
      "\n",
      "\n",
      "The second data:\n",
      "[1, 0, 2, 1, 0, 0, 3, 3, 1, 0, 1, 0, 0, 0, 2, 3, 2, 0, 1, 2, 0, 1, 2, 2, 3, 1, 1, 2, 2, 1, 2, 1, 1, 0, 2, 2, 4, 2, 0, 0, 0, 1, 2, 2, 4, 5, 2, 3, 0, 1, 0, 0, 1, 2, 0, 0, 1, 0, 1, 0, 2, 2, 1, 3, 3, 0, 0, 0, 0, 0, 2, 0, 0, 1, 2, 1, 0, 2, 0, 1, 1, 0, 2, 3, 2, 1, 3, 2, 0, 1, 1, 1, 1, 1, 2, 2, 2, 1, 5, 2, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 2, 1, 2, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 2, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 3, 2, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 3, 1, 1, 0, 0, 1, 3, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 2, 3, 0, 0, 1, 1, 0, 3, 2, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 2, 3, 1, 0, 0, 0, 1, 1, 0, 0, 1, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "print('The first data:')\n",
    "print(data_all_x[0])\n",
    "print('\\n')\n",
    "print('The second data:')\n",
    "print(data_all_x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the y_train.csv file via pandas.read_csv( ) operation\n",
    "\n",
    "The prediction is on the second column (usecols=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y = pd.read_csv(data_path_y, delimiter = ',', header=0, usecols=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the some first rows in the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prediction\n",
       "0           0\n",
       "1           0\n",
       "2           1\n",
       "3           1\n",
       "4           1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_y2 = data_y.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how the data_y2 looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [1], [1], [1], [0], [1], [0], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "print(data_y2[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the preprocessed data via pickle.dump( ) operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('preprocess_ge.p', 'wb') as out_file:\n",
    "    pickle.dump((data_all_x, data_y2), out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Load the dataset\n",
    "\n",
    "#### After storing the preprocessed data into a file, we can begin at here and save our time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_preprocess():\n",
    "    with open('preprocess_ge.p', mode='rb') as in_file:\n",
    "        return pickle.load(in_file)\n",
    "(all_data_x, all_data_y) = load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to scale (normalize) the x_data. \n",
    "Through the output from the data_x.describe() above, we checked that the data in each cell has large variance. It may lead poor prediction performance in neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(all_data_x).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2797477893869934, -0.7624355412019748, -0.7573226403169175, -0.7750980346156938, 0.2726396057415219, -0.2547811711804599, 0.264078318950543, -0.27307126630112866, 0.2623155553711524, -0.7716910458734757, 0.244521952669251, -0.7877368812021848, 0.2513985051900043, -0.2566196924986161, -0.28061523738611954, -0.7779356953053257, 0.2218815061465746, -0.23921559100404186, -0.27703673766121023, -0.27810065758840774, 0.6511812492122052, -0.29572408866663147, -0.30139834475050553, -0.3031409061895812, 0.173234224176646, -0.3085504317898276, 0.6344942605038979, 0.1579002027153852, 0.6075168187927273, 0.5788682222099528, 0.13028855438080347, 0.5872091001625677, -0.3336062370307608, -0.7868838151131863, -0.7824095562175571, 0.10031950557910718, -0.7932474018709039, 0.5339440560790497, 0.9315104255622451, 0.07092520164851879, -0.8006414761856163, 1.8245077284076003, 0.5320932944600699, -0.3512686813274828, 0.5675078136174944, 1.481416908210694, 1.5115510064247073, 1.6065327977212194, 0.220491608974502, 0.7361645609059179, -0.23692694593714161, 0.7031437651597787, -0.7763963991993971, -0.31935015852144544, -0.784177811077067, 0.10373426294312427, -0.3042948892526708, -0.3230948537781977, -0.7239565478023229, -0.7788314899496519, -0.352971151732622, 0.08223310153972929, -0.3482428788491596, -0.35676271651618735, 0.09460025712644106, -0.7666946380760966, 0.10956216158465236, -0.7891494285360433, -0.3284477625995487, -0.7853875685480948, -0.7796745981766893, 0.15644831677051743, -0.31594340319282815, -0.7945064836129556, -0.3071985822509152, 1.1633391170418121, 1.1796460224889176, 0.6818114998104698, -0.2838498934654124, -0.2870441818985706, 0.21614619392235573, -0.781610586095754, -0.7949149233982313, 0.6296686141793223, 0.6610716818215228, -0.6839581493614413, 0.6268873920480846, -0.23227458000138626, -0.25792352953356706, -0.26109652831952335, -0.7903014277677797, -0.791451492259545, 0.27606464009228154, 1.3565982812205928, -0.2533566309105084, -0.24978523137724115, 0.8457556082184885, -0.2537186167778187, 0.3055714982173742, 1.4117340184409333, -0.3455182403451863, 0.12196125728958844, -0.8158248333645115, 0.12735358541744854, -0.8206056379809741, 0.1355644907878256, 0.13065152463634033, -0.33975988705202914, 0.6221817901878445, 0.14889881098117702, -0.8333055627682655, -0.839438771155263, -0.8342740778181387, -0.806427498247215, -0.3338370074511724, -0.8248876094307445, -0.8324075831440332, -0.3088569014494154, -0.33035984483360004, -0.32323577991885993, -0.8320123802696701, -0.31578431808551055, -0.8490792423891989, -0.848617528768052, -0.8569249553179549, -0.8482835631601356, -0.863064027576076, 0.8509763037956076, -0.2910820958086554, -0.8663753610143037, -0.879774783758325, -0.2827868010420771, 0.32257641294219674, -0.9004007032222807, -0.8985585929533909, -0.2706129465674439, -0.26204788263276807, -0.9150984647751397, -0.926424089219681, 1.8455364100960128, 0.4616780240238906, -0.2387012347084727, -0.9530071565611824, -0.9532234432156284, -0.9449058366685339, 1.2320396135821545, 0.5175912939524154, -0.9164186609913386, -0.12054126800478641, 0.6498974914802612, -0.10686292748475663, 0.6253955781906625, -0.9142229031038649, -0.19564629051717325, -0.21256076886694814, -0.9043129976651798, 0.41599082891368333, -0.8492105603041402, -0.8568917392503657, -0.9299785350311016, -0.9277129218866219, 0.41708425319681436, -0.9177418252615984, -0.9186133379295218, -0.8953000749667086, -0.2700863320235793, -0.8853160901355683, -0.2813617039403987, 0.30903689010454916, -0.8797332577185546, -0.876524206023834, -0.8667133234854153, -0.8679815543913364, -0.8602946376343552, -0.8497468395233514, -0.3201959302796448, -0.319404187129365, -0.3156750194892662, 0.1975286730840897, -0.8507005553170648, -0.8384831966839472, -0.8357598654619021, -0.8430939105374987, -0.800043813905835, -0.8166837380466957, -0.8097189699736408, -0.8176120528424072, -0.32248402560950606, -0.8357379020355689, -0.8250203479862026, 0.14380219758959037, -0.3400710867639019, -0.8195796469259105, -0.341604306615417, -0.8300144278164037, -0.8216871866059575, -0.815723297229811, -0.8155057093060135, 0.12301480128998014, -0.34228184928890737, 0.43517669210305826, -0.3557803913389271, 0.4263288147337651, -0.10048957712164806, -0.6241944968254142, -0.6266548843104239, -0.12036347734040725, 0.38928266333897854, 0.3634115465175274, 0.11873646883521563, 0.11364211358407407, 0.3511776690320891, 0.5979256194015371, 0.08679959309401036, -0.6443243413628614, -0.4031857502857523, -0.17073451145817348, -0.17500572044896856, 0.056938512105916536, -0.18537827924383035, 0.04080297676909919, 0.26807365309474723, 0.014817805533198334, -0.4588947147872944, -0.46402032285469486, 0.21533596797067892, 0.8592643007708285, -0.05319983398404793, 0.12765680443983313, -0.10545568224716281, -0.1226579532628307, -0.7486993395572616, -0.7701452208521535, -0.7804049872110653, -0.7999893084907387, -0.6208806215248428, -0.6412564480959986, -0.8390209647905001, -0.8519838196435549, -0.6975418943852054, -0.7151126623451103, -0.9046592216528118, -0.5599289546759411, -0.7356429272281557, -0.5299025441040399, -0.9026990283461934, -0.8652047883863444, -0.8516493745821498, -0.3201586442569996, -0.7907677035101249, -0.7961418817532058, -0.5673482063700814, -0.6149132831377562, -0.8740293768025381, -0.8913085421233701, -0.5302735063546655, -0.001755994170483356, -0.558447582628839, -0.9051363392960875, -0.8937974980430322, -0.87160474896751, -0.6733329857799413, -0.8318494831932836, -0.8085525671255852, -0.8004736353834404, -0.21647571163370216, -0.7744560943661982, -0.7623574634932813, -0.7534168454627411, -0.7463534642842274, -0.7332598814696724, -0.7259849543395807, -0.7184123041956444, -0.7078191883365859, -0.6924721593292483, -0.6865532975667605, -0.6863433229497452, -0.44959307891745365, -0.43999978129576317, -0.6772189605495973, -0.6675880769432395, -0.6659851024469772, -0.6580612807647778, -0.6446729995159415, -0.648481503863169, -0.6434167113805942, -0.6305078864509176, -0.6249876651568488, -0.6326669426831549, -0.12379853813330892, -0.6313501931051512, -0.6305043736990692, -0.6298458610022137, -0.37152131797102217, -0.6258906860403214, -0.6181648483741806, -0.61629950323601, -0.35789393555420024, -0.3573690399977299, -0.6157484997890948, -0.1651635294655095, -0.1678545346868292, -0.16197678049006714, -0.3594267586178743, -0.36119884704434374, -0.36701827713881563, -0.3731265273502716, 0.025896958643625016, 0.22889614694031477, 0.03194590450701767, 0.4553547397492853, -0.1837158156162596, 0.25224249595731196, 1.526630248098889, 1.296874163403137, 2.4966285379022515, 2.6409185978383625, 1.9715629579542464, 1.9219118027275404, 0.8985546099460046, 0.33905490679743994, 0.4971078017984239, 0.47787454789310013, -0.04324669180716915, 0.11150534323270408, -0.06385729723166518, 0.24521036969609333, 0.22625378243571445, 0.20336394539045557, -0.25863226393736743, -0.2707160995569672, -0.27861875613513115, -0.039777853762470686, -0.18845804029860824, -0.44002756636799245, -0.34605398345243077, -0.4675999653791947, -0.38878614091823294, -0.4190976891299644, -0.37423667959082363, -0.48880282197516794, -0.5251214659928466, -0.5096203296041925, -0.5626142236992149, -0.6620078695251811, -0.6557904471753611, -0.7707022998542997, -0.7866651225123095, -0.8023687748110246, -0.7779682092880307, -0.8253869602032872, -0.7649715677145552, -0.785998241483706, -0.7335176367445185, -0.7485059516054754, -0.7147178701164781, -0.6227355052155177, -0.5795591843892524, -0.5960109066485554, -0.561394344386891, -0.4517030186705737, -0.41822842396129356, -0.48265482853016445, -0.46933420116843366, -0.4567984854328004, -0.4396509191863959, -0.19773186915025542, -0.301913995810212, -0.2873359971176721, -0.27750263041344614, -0.12455525673449042, -0.106384079204044, -0.4032194483811101, -0.40289333577828523, -0.2422434285294116, -0.40953635115870934, -0.4028542921564999, -0.40181385816694093, -0.21200250336098211, -0.20337884731552433, -0.3864820643224474, -0.39114653452855797, -0.38708566192546634, -0.3841801825191145, -0.3820188050584187, -0.3880322348192719, -0.39314295625639956, -0.17975651699481218, -0.18172572942841098, -0.3951275121234113, 0.2552159553973276, 0.04058339355790778, -0.3819182351112722, -0.38866241117450634, -0.38671223457618714, -0.16513818619504111, -0.1633180912321066, -0.38840568891276367, -0.377022699001404, -0.3775458783520101, -0.770053249897787, -0.08899367007413742, -0.0944489793485515, -0.0973965649733838, -0.7872318408411769, -0.09699435627135902, -0.09814741293035525, 0.6188632416734685, -0.09699172540564116, 0.6175407930216152, 0.6169392316836237, 0.6237543042853244, 0.6375686171359802, 0.5815425798403033, 0.624532499209457, -0.09407657857239128, -0.08286602972628328, -0.6911783390150615, -0.09520974610817873, -0.8195259344365365, 2.050512273470351, -0.07892102606415397, -0.7974644276141192, -0.08272508418862405, -0.8247706537039415, -0.8472800639990515, -0.08704983394837136, -0.09529613522846835, -0.08394412928179969, -0.0848892129493409, 0.6435733691287311, -0.10887685072553283, -0.8479913368928803, -0.10937957941442707, -0.11358434131878445, -0.8453831986373901, -0.8472309015386369, -0.8574720364856013, -0.12626302778164583, -0.13091611969777003, 0.6122270832607474, -0.12812102043096682, -0.12383911805458322, -0.09984476957509189, -0.08392553984651754, -0.0659719018653683, 1.4202870234169036, 1.5654457544503713, -0.7716496679960438, 2.639984890874161, 0.9522968207202455, 0.03744407270981141, -0.8042753834590215, -0.8288162662953461, -0.8359761331136973, -0.7502373872373748, 0.4446426454431434, 0.49118643812039914, -0.6664563415646271, -0.8480181060505446, -0.853907848936358, -0.8634555720158495, -0.8620894943372842, -0.8619048024800957, -0.10156896511325533, -0.1069024412685513, 0.613043472890771, -0.8583974447495379, -0.10416913627917206, -0.09630432583664725, -0.8555056974827677, -0.83914273017195, -0.8410519015436125, -0.8449644199719045, -0.842855845652721, -0.8382365079792696, -0.08392854723525438, -0.0975407954771827, -0.09848006930188584, -0.08599982513528216, -0.8168243962701073, -0.08248710296884176, -0.8153106651001738, -0.6590648237995979, -0.7203728503746136, -0.6433664799686728, -0.06392855766799134, 0.5246061969438978, -0.06536603082324546, -0.8041376675135726, -0.7870442478200004, -0.07311859057925191, -0.7904825861557024, -0.07986407845663436, -0.08702335541422823, -0.0818703869431076, -0.08589786870580982, -0.7909460696771599, -0.7923856932646361, -0.08788466785119038], [-0.2536710652685408, -0.7624355412019748, 0.2671502421245491, -0.24577529741507553, -0.7793973478860551, -0.7903888871361241, 0.7948743689179832, 0.7820774694875121, -0.26706752490022706, -0.7716910458734757, -0.25699095290470286, -0.7877368812021848, -0.7956444891260839, -0.7374129094787818, 0.23053956215794122, 0.7139319865560062, 0.2218815061465746, -0.6609202160834576, -0.27703673766121023, 0.19821708190462334, -0.7549743279807941, -0.29572408866663147, 0.18562590323671965, 0.1783090355894613, 0.6528573175747083, -0.3085504317898276, -0.31474912135232735, 0.1579002027153852, 0.1467763966649292, -0.3157200433510372, 0.13028855438080347, -0.32932480932411307, -0.3336062370307608, -0.7868838151131863, 0.10995340894623987, 0.10031950557910718, 0.9822646160485045, 0.0928833653512099, -0.7828274565189725, -0.7976210597412114, -0.8006414761856163, -0.3632344254093186, 0.08531611927765033, 0.08934659047130795, 1.0211917833576543, 1.481416908210694, 0.15300626590078872, 0.6533118469697027, -0.7534957152218076, -0.23781939548746375, -0.7287909703152939, -0.7531444029408687, -0.30029556593459966, 0.13942497875516494, -0.784177811077067, -0.759649269265572, -0.3042948892526708, -0.7229289311981123, -0.33333432602738405, -0.7788314899496519, 0.08682272375656787, 0.08223310153972929, -0.3482428788491596, 0.5187476900215562, 0.5220542931938295, -0.7666946380760966, -0.7817276057588419, -0.7891494285360433, -0.7829619363130439, -0.7853875685480948, 0.1339627814264765, -0.7909909422015052, -0.7952599113187167, -0.3084358858986568, 0.1763807657368272, -0.3050189506711992, -0.789822296458887, 0.19460102105804136, -0.7833857043884617, -0.2870441818985706, -0.28251647188331735, -0.781610586095754, 0.2311130493300851, 0.6296686141793223, 0.20132610592076464, -0.24299337532272372, 0.6268873920480846, 0.21636853728955385, -0.7679408778705976, -0.26109652831952335, -0.26233138553195984, -0.26471292013425796, -0.2650569993797412, -0.2570020146136616, 0.29047861023853827, 0.28742647805006155, 0.29603576296734285, -0.2537186167778187, 1.9590745791362, 0.30804838833249426, -0.8122677781402844, -0.8180935307289667, -0.8158248333645115, -0.34556494460394027, -0.35066931016531644, -0.34801444029936146, -0.8259153117673143, -0.8312753813532877, -0.3468385790410991, -0.8377071316989795, -0.8333055627682655, -0.3414802448244613, -0.34113615763590877, -0.806427498247215, -0.8352887545053391, -0.33090568383576113, 0.17915462403551197, -0.3088569014494154, 0.17668963452493863, -0.829435825635238, -0.8320123802696701, -0.8372089465301823, -0.8490792423891989, -0.3180388380516436, -0.8569249553179549, -0.30352882606667153, -0.3038779829310745, -0.2888358287426662, -0.2910820958086554, -0.29592623841431676, -0.879774783758325, -0.8849531396197124, -0.8914461303583054, -0.9004007032222807, 0.34946184043823686, -0.2706129465674439, -0.9117135026405192, -0.25119761549463754, -0.926424089219681, -0.9311961307451023, -0.9349002533759816, -0.9492526802026477, -0.9530071565611824, -0.22042966780014878, -0.21695226050872066, -0.9317500446144726, -0.9055090588269085, -0.17650818779404587, -0.12054126800478641, -0.10996830854781532, -0.10686292748475663, -0.8998033395638706, -0.9142229031038649, -0.9261943514427649, -0.9265528525772937, -0.9043129976651798, -0.2097127151104436, -0.8492105603041402, -0.8568917392503657, -0.9299785350311016, -0.9277129218866219, -0.9243945422653665, -0.9177418252615984, -0.9186133379295218, -0.2713702072412593, -0.8817106156423459, -0.8853160901355683, -0.8797528570123772, -0.8751839075368623, -0.2943910373012548, -0.29749697466514324, -0.2897861939590577, -0.8679815543913364, 0.23767474040635939, -0.8497468395233514, -0.8563951278983645, -0.319404187129365, -0.3156750194892662, -0.3210761934311917, 0.20154787499563973, -0.8384831966839472, -0.8357598654619021, -0.8430939105374987, -0.3301883711807675, -0.8166837380466957, -0.8097189699736408, -0.328630026028539, -0.8104802277558172, -0.33616633619805547, -0.33467567582199187, -0.8237307286214924, -0.8184939793358126, -0.3413885831486789, -0.341604306615417, 0.13668659243383222, -0.8216871866059575, -0.815723297229811, -0.8155057093060135, -0.3463997373929531, -0.34228184928890737, -0.3521478875459494, -0.61845128672766, -0.6189057793372217, -0.6285116679695342, -0.6241944968254142, -0.6266548843104239, -0.3743948861745637, -0.6290106838131954, -0.6205335158287437, -0.6302997424276771, -0.6309621722478241, -0.1435220084718362, -0.3923816929074421, -0.3994853920943944, -0.6443243413628614, -0.4031857502857523, -0.6548183547118248, -0.41508045301029717, -0.4197810661369153, -0.42465331917804006, -0.4349757094278143, -0.4416409166047161, -0.45202597611802553, -0.4588947147872944, -0.6961054229646738, -0.6984698593863952, -0.7098314367256583, -0.4924423982298399, -0.7197403091603451, -0.7288102717358952, -0.7395998349025882, -0.7486993395572616, -0.5723330734116082, -0.5881408317561484, -0.7999893084907387, -0.8028979332817613, -0.8193097769842077, -0.8390209647905001, -0.8519838196435549, -0.8695584604762492, -0.8874819411212266, -0.9046592216528118, -0.9156052899505569, -0.9192809000184573, -0.9129005555139237, -0.9026990283461934, -0.8652047883863444, -0.8516493745821498, -0.5661351566023053, -0.5315915216353782, -0.5339166501424433, -0.8208727778542446, -0.8485000800514093, -0.8740293768025381, -0.8913085421233701, -0.9086040097596599, -0.9142247770541814, -0.9133649155657392, -0.9051363392960875, -0.8937974980430322, -0.87160474896751, -0.8470800493493862, -0.8318494831932836, -0.6289708617047636, -0.8004736353834404, -0.7849539017944501, -0.7744560943661982, -0.7623574634932813, -0.7534168454627411, -0.3233738535300685, -0.5186955611960867, -0.508262581136877, -0.4969211535870044, -0.4846939693680368, -0.6924721593292483, -0.6865532975667605, -0.6863433229497452, -0.6796640910468909, -0.6739682787156627, -0.6772189605495973, -0.6675880769432395, -0.6659851024469772, -0.4150411063677306, -0.4040375554396644, -0.648481503863169, -0.6434167113805942, -0.383607860614066, -0.1314673186523203, -0.6326669426831549, -0.6270855853883072, -0.6313501931051512, -0.6305043736990692, -0.6298458610022137, -0.11234180683072038, -0.6258906860403214, -0.6181648483741806, -0.61629950323601, -0.6135796376098769, -0.3573690399977299, -0.6157484997890948, -0.36003988223702843, -0.36365321413131513, -0.3539855415624617, -0.16591715020407194, -0.16938937700039097, -0.36701827713881563, -0.3731265273502716, -0.37753805604101615, -0.17630040468823902, -0.17468831908996169, -0.3903879385940153, 0.2503457353171825, 0.0323486126099621, -0.1898242366940895, -0.1907829888190468, -0.19397816807237664, -0.19830987368319292, -0.19414933329868986, -0.3893227456634766, -0.3838863621019384, -0.20105879237528732, -0.20231879891925494, -0.20960773050658912, -0.37909173826354553, -0.3877217885979886, -0.22700368357588813, -0.3940291210015507, -0.3940338856060752, -0.399939027811557, -0.4040119868599075, -0.4114452120062635, -0.41165231781230743, -0.42196610537272405, -0.4286055677857666, -0.3266659468360184, -0.34605398345243077, -0.4675999653791947, -0.4792334440659574, -0.5028663760901242, -0.5297272277190225, -0.5601799078855965, -0.5900006749872793, -0.6271508729724893, -0.6688642999710128, -0.7099113067677473, -0.7416123120693228, -0.7707022998542997, -0.7866651225123095, -0.8023687748110246, -0.8234702138689819, -0.8253869602032872, -0.7649715677145552, -0.7078440752544856, -0.7335176367445185, -0.6615741683563132, -0.6664587612278591, -0.5694321355751887, -0.5204979415608093, -0.5960109066485554, -0.561394344386891, -0.4517030186705737, -0.33568719781020406, -0.48265482853016445, -0.46933420116843366, -0.4567984854328004, -0.3289698188189542, -0.43523404055823756, -0.4260118437416518, -0.2873359971176721, -0.4132214173489591, -0.4022913285227522, -0.3994821072219465, -0.24925172322683078, -0.2431083875255833, -0.2422434285294116, -0.23389600335799438, -0.04774535414526286, -0.40181385816694093, -0.21200250336098211, -0.20337884731552433, -0.2025137028815726, -0.39114653452855797, -0.193223587637299, -0.19111760945789272, -0.3820188050584187, -0.3880322348192719, -0.39314295625639956, -0.3947506983236657, -0.4049900042775814, -0.3951275121234113, -0.39400413046308497, -0.3803368001756502, -0.3819182351112722, -0.38866241117450634, 0.056861021158054206, -0.16513818619504111, -0.3836317633925013, -0.38840568891276367, -0.16394983425553755, -0.3775458783520101, -0.770053249897787, -0.7769951583906713, -0.7747012794473047, -0.7898602052914963, 0.604512086025546, -0.7942820103391374, -0.09814741293035525, -0.09934835221948354, -0.8044419693558785, -0.7926670199764251, -0.06921346548084613, 0.6237543042853244, -0.09624660780460645, -0.7573195743118073, -0.8148015907233368, -0.09407657857239128, -0.8140229928667375, -0.08184747766304173, -0.09520974610817873, -0.8195259344365365, -0.09205835708960655, -0.82409912887112, 0.6229918513994698, 1.3981973766909637, -0.08874174795212413, -0.0890224328931559, -0.84390928296046, -0.8462684782375429, -0.08394412928179969, 1.4004322136275453, -0.8424079357185666, -0.86288312688156, -0.11453069611835914, -0.8655147522860351, -0.11358434131878445, -0.8453831986373901, -0.8472309015386369, -0.8574720364856013, -0.8558089177811923, -0.13091611969777003, -0.12663476572161353, 0.6152113142912217, -0.12383911805458322, 0.6640367802620117, 1.4495104300925081, -0.817682645342357, -0.7534497957717189, -0.004155871272498317, 0.04655049010575973, -0.7619470820125499, 1.8157653924999035, 0.8598858683288598, -0.8042753834590215, -0.8288162662953461, -0.8359761331136973, -0.08010437262208017, -0.08280421871244689, 0.49118643812039914, -0.09926335178016703, -0.8480181060505446, -0.853907848936358, -0.8634555720158495, -0.11983451408636148, -0.11906538207578328, -0.7596423646022755, -0.7892537615667855, -0.10907566431549622, -0.8583974447495379, -0.8611184911749855, -0.09630432583664725, -0.09859187474704283, -0.10100792122440137, -0.08810695759941174, -0.8449644199719045, -0.842855845652721, -0.08480649137496331, -0.831703894725424, -0.8516192867224016, -0.09848006930188584, -0.08599982513528216, -0.08539978652959888, 0.6614341489079872, -0.8153106651001738, -0.6590648237995979, -0.7203728503746136, -0.6433664799686728, 0.5139661974334949, 1.1086544361901596, -0.06536603082324546, -0.8041376675135726, -0.7870442478200004, -0.7938323876000771, -0.07557645097166715, -0.07986407845663436, -0.8087998195756236, -0.797716203890607, -0.08589786870580982, 1.3200156485149845, 0.6063936569636915, -0.7829276104843282]]\n"
     ]
    }
   ],
   "source": [
    "print(scaled_x[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "The label in a binary or n-ary classification problem can be encoded via One-Hot Encoding method.\n",
    "One-Hot encoding for binary classification problem can be seen as a method encoding label in 2-dimensional vector. \n",
    "\n",
    "Example: \n",
    "\n",
    "|Prediction |\n",
    "|---|\n",
    "|0|\n",
    "|1|\n",
    "|0|\n",
    "|0|\n",
    "\n",
    "- 1-dimensional vector: [[0], [1], [0], [0]]\n",
    "\n",
    "- One-hot encoding (2-dimensional vector): [[1, 0], [0, 1], [1, 0], [1, 0]]\n",
    "\n",
    "Each dimension in the 2-d vector denotes each element, thus, the *n* is larger, more zeros in the encoding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [1], [1], [1]]\n"
     ]
    }
   ],
   "source": [
    "#all_data_y_list = all_data_y.tolist()\n",
    "print(all_data_y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "data_y = OneHotEncoder(sparse=False).fit_transform(all_data_y).tolist()\n",
    "print(data_y[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of the data  Training / Development / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the X-Y pair and the number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15485\n"
     ]
    }
   ],
   "source": [
    "n_total = len(scaled_x)\n",
    "print(n_total)\n",
    "n_total_y = len(data_y)\n",
    "assert n_total == n_total_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Warning!!: In the case you want to use small pieces of the data set, run this cell below, or, skip this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using only 1/n of the whole data\n",
    "'''\n",
    "import math\n",
    "_n=3\n",
    "scaled_x = scaled_x[:math.floor(n_total/_n)]\n",
    "data_y = data_y[:math.floor(n_total/_n)]\n",
    "\n",
    "n_total = len(scaled_x)\n",
    "print(n_total)\n",
    "n_total_y = len(data_y)\n",
    "assert n_total == n_total_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide the whole set into 7:1:2 for training, dev, and test, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train:    10836\n",
      "#dev:    1548\n",
      "#test:    3101\n"
     ]
    }
   ],
   "source": [
    "x_data_train = scaled_x[:n_total//10*7]\n",
    "y_data_train = data_y[:n_total//10*7]\n",
    "\n",
    "x_data_dev = scaled_x[n_total//10*7:n_total//10*8]\n",
    "y_data_dev = data_y[n_total//10*7:n_total//10*8]\n",
    "\n",
    "x_data_test = scaled_x[n_total//10*8:]\n",
    "y_data_test = data_y[n_total//10*8:]\n",
    "print ('#train: ',' ', len(x_data_train))\n",
    "print ('#dev: ',' ', len(x_data_dev))\n",
    "print ('#test: ',' ', len(x_data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: Build the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "* We will design a neural network graph consisting of one input layer, three hidden layers, and one output layer.\n",
    "* In the cell below, set your hyper parameters for the node number for each layer\n",
    "* Input layer (size 500)  Layer 1 (size ?)  Activation  Dropout  Layer 2 (size ?)  Activation  Dropout  Layer 3 (size ?)  Activation  Output layer (size ?)  Softmax\n",
    "* Also, we can control the learning rate and *keep probability* in the dropout layer. Set your hyperparameters for the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=100 # the number of nodes in the layer 1\n",
    "#l2=24\n",
    "#l3=16\n",
    "lr=0.001 #learning rate\n",
    "kp=0.4 #keep probability (dropout layer)\n",
    "n_iter=5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the graph structure in the TF code. \n",
    "You may use this structure and can modify the structure.\n",
    "* tf.name_scope('name') will make a group in the graph. It is very useful tip making your graph compact when you see your graph in the tensorboard.\n",
    "* If you want to see scalar (cost), use **tf.summary.scalar('name', variable)**\n",
    "* If you want to see histogram (weights, bias), use **tf.summary.histogram('name', variable)**\n",
    "* Before running the training session, type **your_summary_name=tf.summary.merge_all()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "single layer\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "ncol = len(scaled_x[0])\n",
    "nclass = 2\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, ncol])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, nclass])\n",
    "\n",
    "\n",
    "with tf.name_scope('layer1'):\n",
    "    #W1 = tf.get_variable(\"w1\", shape=[ncol, l1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    W1 = tf.Variable(tf.random_normal([ncol, l1]), name='w1')\n",
    "    b1 = tf.Variable(tf.random_normal([l1]), name='b1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1)+b1, name='layer1')\n",
    "    layer1 = tf.nn.dropout(layer1, kp)\n",
    "    tf.summary.histogram(\"w\", W1)\n",
    "    tf.summary.histogram(\"b\", b1)\n",
    "\n",
    "\n",
    "with tf.name_scope('FC'):\n",
    "    #FC\n",
    "    #Wf = tf.get_variable(\"wf\", shape=[l1, nclass], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    Wf = tf.Variable(tf.random_normal([l1, nclass]), name='wf')\n",
    "    bf = tf.Variable(tf.random_normal([nclass]), name='bf')\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer1, Wf)+bf)\n",
    "    tf.summary.histogram(\"w\", Wf)\n",
    "    tf.summary.histogram(\"b\", bf)\n",
    "\n",
    "\n",
    "#W = tf.Variable(tf.random_normal([ncol, 1]), name='w')\n",
    "#b = tf.Variable(tf.random_normal([1]), name='b')\n",
    "#hypothesis = tf.nn.softmax(tf.nn.relu(tf.matmul(X, W)+b))\n",
    "\n",
    "with tf.name_scope('trainer'):\n",
    "    #cost_2 = tf.reduce_mean(-tf.reduce_sum(Y_2*tf.log(hypothesis_2), axis=1))\n",
    "    logit = tf.matmul(layer1, Wf)+bf\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels = Y))    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)        \n",
    "    \n",
    "with tf.name_scope('predictor'):\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean (tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2 layers\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "ncol = len(scaled_x[0])\n",
    "nclass = 2\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, ncol])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, nclass])\n",
    "    #tf.summary.histogram(\"x2\", X_2)\n",
    "    #tf.summary.histogram(\"y2\", Y_2)\n",
    "\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.get_variable(\"w1\", shape=[ncol, l1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #W1 = tf.Variable(tf.random_normal([ncol, l1]), name='w1')\n",
    "    b1 = tf.Variable(tf.random_normal([l1]), name='b1')\n",
    "    layer1 = tf.nn.relu(tf.matmul(X, W1)+b1, name='layer1')\n",
    "    layer1 = tf.nn.dropout(layer1, kp)\n",
    "    tf.summary.histogram(\"w\", W1)\n",
    "    tf.summary.histogram(\"b\", b1)\n",
    "\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.get_variable(\"w2\", shape=[l1, l2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #W2 = tf.Variable(tf.random_normal([l1, l2]), name='w2')\n",
    "    b2 = tf.Variable(tf.random_normal([l2]), name='b2')\n",
    "    layer2 = tf.nn.tanh(tf.matmul(layer1, W2)+b2, name='layer2')\n",
    "    layer2 = tf.nn.dropout(layer2, kp)\n",
    "    tf.summary.histogram(\"w\", W2)\n",
    "    tf.summary.histogram(\"b\", b2)\n",
    "\n",
    "with tf.name_scope('FC'):\n",
    "    #FC\n",
    "    Wf = tf.get_variable(\"wf\", shape=[l2, nclass], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #Wf = tf.Variable(tf.random_normal([l2, nclass]), name='wf')\n",
    "    bf = tf.Variable(tf.random_normal([nclass]), name='bf')\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer2, Wf)+bf)\n",
    "    tf.summary.histogram(\"w\", Wf)\n",
    "    tf.summary.histogram(\"b\", bf)\n",
    "\n",
    "\n",
    "#W = tf.Variable(tf.random_normal([ncol, 1]), name='w')\n",
    "#b = tf.Variable(tf.random_normal([1]), name='b')\n",
    "#hypothesis = tf.nn.softmax(tf.nn.relu(tf.matmul(X, W)+b))\n",
    "\n",
    "with tf.name_scope('trainer'):\n",
    "    #cost_2 = tf.reduce_mean(-tf.reduce_sum(Y_2*tf.log(hypothesis_2), axis=1))\n",
    "    logit = tf.matmul(layer2, Wf)+bf\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels = Y))    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)        \n",
    "    \n",
    "with tf.name_scope('predictor'):\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean (tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "3 layers\n",
    "'''\n",
    "tf.reset_default_graph()\n",
    "ncol = len(scaled_x[0])\n",
    "nclass = 2\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=[None, ncol])\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, nclass])\n",
    "    #tf.summary.histogram(\"x2\", X_2)\n",
    "    #tf.summary.histogram(\"y2\", Y_2)\n",
    "\n",
    "with tf.name_scope('layer1'):\n",
    "    W1 = tf.get_variable(\"w1\", shape=[ncol, l1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #W1 = tf.Variable(tf.random_normal([ncol, l1]), name='w1')\n",
    "    b1 = tf.Variable(tf.random_normal([l1]), name='b1')\n",
    "    layer1 = tf.nn.tanh(tf.matmul(X, W1)+b1, name='layer1')\n",
    "    layer1 = tf.nn.dropout(layer1, kp)\n",
    "    tf.summary.histogram(\"w\", W1)\n",
    "    tf.summary.histogram(\"b\", b1)\n",
    "\n",
    "\n",
    "with tf.name_scope('layer2'):\n",
    "    W2 = tf.get_variable(\"w2\", shape=[l1, l2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #W2 = tf.Variable(tf.random_normal([l1, l2]), name='w2')\n",
    "    b2 = tf.Variable(tf.random_normal([l2]), name='b2')\n",
    "    layer2 = tf.nn.tanh(tf.matmul(layer1, W2)+b2, name='layer2')\n",
    "    layer2 = tf.nn.dropout(layer2, kp)\n",
    "    tf.summary.histogram(\"w\", W2)\n",
    "    tf.summary.histogram(\"b\", b2)\n",
    "\n",
    "with tf.name_scope('layer3'):\n",
    "    W3 = tf.get_variable(\"w3\", shape=[l2, l3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #W3 = tf.Variable(tf.random_normal([l2, l3]), name='w3')\n",
    "    b3 = tf.Variable(tf.random_normal([l3]), name='b3')\n",
    "    layer3 = tf.nn.tanh(tf.matmul(layer2, W3)+b3, name='layer3')\n",
    "    layer3 = tf.nn.dropout(layer3, kp)\n",
    "    tf.summary.histogram(\"w\", W3)\n",
    "    tf.summary.histogram(\"b\", b3)\n",
    "\n",
    "with tf.name_scope('FC'):\n",
    "    #FC\n",
    "    Wf = tf.get_variable(\"wf\", shape=[l3, nclass], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    #Wf = tf.Variable(tf.random_normal([l3, nclass]), name='wf')\n",
    "    bf = tf.Variable(tf.random_normal([nclass]), name='bf')\n",
    "    hypothesis = tf.nn.softmax(tf.matmul(layer3, Wf)+bf)\n",
    "    tf.summary.histogram(\"w\", Wf)\n",
    "    tf.summary.histogram(\"b\", bf)\n",
    "\n",
    "\n",
    "#W = tf.Variable(tf.random_normal([ncol, 1]), name='w')\n",
    "#b = tf.Variable(tf.random_normal([1]), name='b')\n",
    "#hypothesis = tf.nn.softmax(tf.nn.relu(tf.matmul(X, W)+b))\n",
    "\n",
    "with tf.name_scope('trainer'):\n",
    "    #cost_2 = tf.reduce_mean(-tf.reduce_sum(Y_2*tf.log(hypothesis_2), axis=1))\n",
    "    logit = tf.matmul(layer3, Wf)+bf\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels = Y))    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cost)        \n",
    "    \n",
    "with tf.name_scope('predictor'):\n",
    "    \n",
    "    correct_pred = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean (tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    \n",
    "\n",
    "tf.summary.scalar('cost', cost)\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing your graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you want to save tensorboard log file, type like this just after sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  **file_name = tf.summary.FileWriter('log_folder_name', sess.graph)**\n",
    "\n",
    "* If your want to save the variables during the training, run the code\n",
    "\n",
    "  **summary = sess.run([your_summary_name], feed_dict={....})**\n",
    "  **file_name.add_summary(summary, step)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tcost: 130.58849 \tacc: 0.500646\n",
      "200 \tcost: 42.2187 \tacc: 0.75323\n",
      "400 \tcost: 28.81076 \tacc: 0.75452197\n",
      "600 \tcost: 20.463728 \tacc: 0.7616279\n",
      "800 \tcost: 15.498976 \tacc: 0.75645995\n",
      "1000 \tcost: 10.226396 \tacc: 0.75323\n",
      "1200 \tcost: 7.710492 \tacc: 0.747416\n",
      "1400 \tcost: 4.971384 \tacc: 0.7609819\n",
      "1600 \tcost: 2.8837857 \tacc: 0.7403101\n",
      "1800 \tcost: 1.2761321 \tacc: 0.74418604\n",
      "2000 \tcost: 0.43280473 \tacc: 0.7629199\n",
      "2200 \tcost: 0.3650502 \tacc: 0.7861757\n",
      "2400 \tcost: 0.3609709 \tacc: 0.8049096\n",
      "2600 \tcost: 0.36142045 \tacc: 0.8068476\n",
      "2800 \tcost: 0.3596309 \tacc: 0.81201553\n",
      "3000 \tcost: 0.3534212 \tacc: 0.8036176\n",
      "3200 \tcost: 0.3525864 \tacc: 0.81266147\n",
      "3400 \tcost: 0.3556942 \tacc: 0.81459945\n",
      "3600 \tcost: 0.349688 \tacc: 0.81072354\n",
      "3800 \tcost: 0.35133058 \tacc: 0.81718343\n",
      "4000 \tcost: 0.35070935 \tacc: 0.81395346\n",
      "4200 \tcost: 0.3473162 \tacc: 0.81524545\n",
      "4400 \tcost: 0.34376583 \tacc: 0.8217054\n",
      "4600 \tcost: 0.34425804 \tacc: 0.81201553\n",
      "4800 \tcost: 0.34753305 \tacc: 0.8197674\n",
      "5000 \tcost: 0.3393209 \tacc: 0.80943155\n",
      "final accuracy:  0.80943155\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #for 1 layer\n",
    "    fw2 = tf.summary.FileWriter('./tb/gene_expression/train/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'), sess.graph)\n",
    "    fw_val = tf.summary.FileWriter('./tb/gene_expression/validation/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'))\n",
    "    fw_test = tf.summary.FileWriter('./tb/gene_expression/test/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'))\n",
    "    \n",
    "    #for 2 layers\n",
    "    #fw2 = tf.summary.FileWriter('./tb/gene_expression/train/l1_{}-l2_{}-lr_{}-kp{}-init_{}'.format(l1,l2,lr,kp,'xavier_fcx'), sess.graph)\n",
    "    #fw_val = tf.summary.FileWriter('./tb/gene_expression/validation/l1_{}-l2_{}-lr_{}-kp{}-init_{}'.format(l1,l2,lr,kp,'xavier_fcx'))\n",
    "    #fw_test = tf.summary.FileWriter('./tb/gene_expression/test/l1_{}-l2_{}-lr_{}-kp{}-init_{}'.format(l1,l2,lr,kp,'xavier_fcx'))\n",
    "    \n",
    "    #for 3 layers    \n",
    "    #fw2 = tf.summary.FileWriter('./tb/gene_expression/train/l1_{}-l2_{}-l3_{}-lr_{}-kp{}-init_{}_{}'.format(l1,l2,l3,lr,kp,'xavier', 'sf'), sess.graph)\n",
    "    #fw_val = tf.summary.FileWriter('./tb/gene_expression/validation/l1_{}-l2_{}-l3_{}-lr_{}-kp{}-init_{}_{}'.format(l1,l2,l3,lr,kp,'xavier', 'sf'))\n",
    "    #fw_test = tf.summary.FileWriter('./tb/gene_expression/test/l1_{}-l2_{}-l3_{}-lr_{}-kp{}-init_{}_{}'.format(l1,l2,l3,lr,kp,'xavier', 'sf'))\n",
    "\n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        \n",
    "        summary, _, c = sess.run([merged, optimizer, cost], feed_dict={X:x_data_train, Y:y_data_train})\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            summary_v, acc = sess.run([merged, accuracy], feed_dict={X:x_data_dev, Y:y_data_dev})\n",
    "            print(step, '\\tcost:', c, '\\tacc:', acc)\n",
    "            #print(summary_v)\n",
    "            fw_val.add_summary(summary_v, step)\n",
    "            \n",
    "        fw2.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        \n",
    "    summary_t, acc_t = sess.run([merged, accuracy], feed_dict={X:x_data_test, Y:y_data_test})    \n",
    "    fw_test.add_summary(summary_t, step)\n",
    "    print('final accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (<ipython-input-26-7beacd7ca707>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-26-7beacd7ca707>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #for 1 layer\n",
    "    fw2 = tf.summary.FileWriter('./tb/gene_expression/train/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'), sess.graph)\n",
    "    fw_val = tf.summary.FileWriter('./tb/gene_expression/validation/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'))\n",
    "    fw_test = tf.summary.FileWriter('./tb/gene_expression/test/l1_{}-lr_{}-kp{}-init_{}'.format(l1,lr,kp,'random'))\n",
    " \n",
    "    for step in range(n_iter+1):\n",
    "\n",
    "        \n",
    "        #summary, _, c = sess.run([merged, optimizer, cost], feed_dict={X:x_data_train, Y:y_data_train})\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X:x_data_train, Y:y_data_train})\n",
    "        \n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            acc = sess.run([accuracy], feed_dict={X:x_data_dev, Y:y_data_dev})\n",
    "            print(step, '\\tcost:', c, '\\tacc:', acc)\n",
    "            #print(summary_v)\n",
    "            fw_val.add_summary(summary_v, step)\n",
    "            \n",
    "        fw2.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    summary_t, acc_t = sess.run([merged, accuracy], feed_dict={X:x_data_test, Y:y_data_test})    \n",
    "    fw_test.add_summary(summary_t, step)\n",
    "    print('final accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson: See your graph in the Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open your CMD (command) console\n",
    "* Activate your working environment first.\n",
    "* Let's move to your workspace folder\n",
    "* Typing like this, tensorboard --logdir=./your/log/folder\n",
    "* you can see the access URL looking like this (https://the_name_of_your-PC:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
